{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computa\u00e7\u00e3o em Nuvem","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#kit-n","title":"KIT-N","text":"<p>Luka Figueiredo</p> <p>Luiz Durand</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 21/03/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"projeto/main/","title":"Relat\u00f3rio do Projeto de API em Nuvem","text":""},{"location":"projeto/main/#objetivo","title":"Objetivo","text":"<p>Este relat\u00f3rio documenta o desenvolvimento, containeriza\u00e7\u00e3o e implanta\u00e7\u00e3o de uma API RESTful em FastAPI, integrada a um banco de dados MySQL, distribu\u00edda via Docker Hub e orquestrada tanto localmente (Docker Compose) quanto em ambiente OpenStack (gerenciado por Juju), com balanceamento de carga via Nginx.</p>"},{"location":"projeto/main/#1-tarefa-1-desenvolvimento-da-api-com-fastapi","title":"1. Tarefa 1 - Desenvolvimento da API com FastAPI","text":""},{"location":"projeto/main/#11-rotas-de-usuario","title":"1.1 Rotas de Usu\u00e1rio","text":"<ol> <li> <p>POST /registrar</p> </li> <li> <p>Recep\u00e7\u00e3o de JSON com <code>nome</code>, <code>email</code> e <code>senha</code> para criar usu\u00e1rio.</p> </li> <li> <p>POST /login</p> </li> <li> <p>Valida\u00e7\u00e3o de credenciais e retorno de JWT.</p> </li> <li> <p>GET /consultar</p> </li> <li> <p>Rota protegida que exige token Bearer.</p> </li> </ol>"},{"location":"projeto/main/#12-autenticacao-e-seguranca","title":"1.2 Autentica\u00e7\u00e3o e Seguran\u00e7a","text":"<ul> <li>Uso de bcrypt (via Passlib) para hashear senhas.</li> <li>Emiss\u00e3o de tokens JWT com <code>python-jose</code>, configurados para expirar em 30 minutos.</li> <li>Substitui\u00e7\u00e3o de OAuth2 por HTTPBearer para simplificar o fluxo de teste no Swagger UI.</li> </ul>"},{"location":"projeto/main/#13-modelagem-de-dados-e-hash-de-senha","title":"1.3 Modelagem de Dados e Hash de Senha","text":"<ul> <li>Fun\u00e7\u00f5es <code>get_password_hash()</code> e <code>verify_password()</code> em <code>app/core/security.py</code>.</li> <li>Cria\u00e7\u00e3o de <code>create_access_token()</code> e <code>decode_access_token()</code> para JWT.</li> </ul>"},{"location":"projeto/main/#2-tarefa-12-modelagem-e-persistencia-com-sqlalchemy-mysql","title":"2. Tarefa 1.2 - Modelagem e Persist\u00eancia com SQLAlchemy + MySQL","text":""},{"location":"projeto/main/#21-configuracao-do-sqlalchemy-appcoredatabasepy","title":"2.1 Configura\u00e7\u00e3o do SQLAlchemy (<code>app/core/database.py</code>)","text":"<ul> <li><code>DATABASE_URL</code> lido de vari\u00e1vel de ambiente.</li> <li>Defini\u00e7\u00e3o de <code>engine</code>, <code>SessionLocal</code> e <code>Base</code> para MySQL via PyMySQL.</li> </ul>"},{"location":"projeto/main/#22-definicao-do-modelo-orm-appmodelspy","title":"2.2 Defini\u00e7\u00e3o do Modelo ORM (<code>app/models.py</code>)","text":"<pre><code>class User(Base):\n    __tablename__ = \"users\"\n    id = Column(Integer, primary_key=True, index=True)\n    nome = Column(String(255), nullable=False)\n    email = Column(String(255), unique=True, index=True, nullable=False)\n    senha = Column(String(255), nullable=False)\n</code></pre>"},{"location":"projeto/main/#23-funcoes-crud","title":"2.3 Fun\u00e7\u00f5es CRUD","text":"<ul> <li><code>create_user(db, user: Usuario)</code>: insere novo usu\u00e1rio com hash de senha.</li> <li><code>get_user_by_email(db, email)</code>: retorna usu\u00e1rio ou <code>None</code>.</li> <li><code>authenticate_user(db, email, senha)</code>: compara senha em texto com hash salvo.</li> </ul>"},{"location":"projeto/main/#24-dependencia-de-sessao-no-fastapi-appapppy","title":"2.4 Depend\u00eancia de Sess\u00e3o no FastAPI (<code>app/app.py</code>)","text":"<ul> <li><code>get_db()</code> retorna <code>SessionLocal</code> para cada requisi\u00e7\u00e3o.</li> <li>Evento de startup com retry para aguardar o MySQL criado pelo Compose.</li> </ul>"},{"location":"projeto/main/#3-tarefa-2-containerizacao-local-via-docker-e-docker-compose","title":"3. Tarefa 2 - Containeriza\u00e7\u00e3o Local via Docker e Docker Compose","text":""},{"location":"projeto/main/#31-dockerfile","title":"3.1 Dockerfile","text":"<ul> <li>Base: <code>python:3.10-slim</code></li> <li>Instala\u00e7\u00e3o de depend\u00eancias via <code>requirements.txt</code> (FastAPI, Uvicorn, SQLAlchemy, PyMySQL, Passlib, python-jose).</li> <li>Comando de inicializa\u00e7\u00e3o:</li> </ul> <pre><code>CMD [\"uvicorn\", \"app.app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n</code></pre>"},{"location":"projeto/main/#32-docker-composeyml","title":"3.2 docker-compose.yml","text":"<pre><code>version: \"3.8\"\n\nservices:\n  db:\n    image: mysql:8.0\n    container_name: mysql_projeto\n    restart: always\n    environment:\n      MYSQL_ROOT_PASSWORD: rootpassword\n      MYSQL_DATABASE: p_cloud\n      MYSQL_USER: cloud\n      MYSQL_PASSWORD: cloud-n123\n    healthcheck:\n      test: [\"CMD\",\"mysqladmin\",\"ping\",\"-h\",\"localhost\",\"-ucloud\",\"-pcloud-n123\"]\n      interval: 5s\n      retries: 5\n      timeout: 2s\n    volumes:\n      - mysql_data:/var/lib/mysql\n    ports:\n      - \"3307:3306\"\n\n  app:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    image: luizdurand/api-fastapi-nuvem:latest\n    container_name: api_fastapi\n    ports:\n      - \"8080:8080\"\n    environment:\n      DATABASE_URL: mysql+pymysql://cloud:cloud-n123@db:3306/p_cloud\n      SECRET_KEY: sua_chave_jwt_super_secreta\n    depends_on:\n      db:\n        condition: service_healthy\n\nvolumes:\n  mysql_data:\n</code></pre>"},{"location":"projeto/main/#33-testes-locais","title":"3.3 Testes Locais","text":"<ol> <li>Executar: <code>docker compose up -d --build</code>.</li> <li>Acessar: <code>http://localhost:8080/docs</code> para validar CRUD e endpoint protegido.</li> <li>No MySQL Workbench, configurar conex\u00e3o em <code>127.0.0.1:3307</code> (usu\u00e1rio <code>cloud</code>, schema <code>p_cloud</code>).</li> </ol>"},{"location":"projeto/main/#4-tarefa-3-integracao-com-ambiente-openstack-via-juju","title":"4. Tarefa 3 - Integra\u00e7\u00e3o com Ambiente OpenStack (via Juju)","text":""},{"location":"projeto/main/#41-provisionamento-de-mysql-innodb-cluster","title":"4.1 Provisionamento de MySQL InnoDB Cluster","text":"<ul> <li>Charm: <code>mysql-innodb-cluster/8.0</code> com 3 unidades (R/W/R/O).</li> <li>Exposi\u00e7\u00e3o do MySQL Router em <code>172.16.0.152:3306</code>.</li> </ul>"},{"location":"projeto/main/#42-configuracao-das-vms-de-api","title":"4.2 Configura\u00e7\u00e3o das VMs de API","text":"<ul> <li>Inst\u00e2ncias: Ubuntu 22.04 (<code>api-vm1</code>, <code>api-vm2</code>) criadas via Juju.</li> <li>Instalar Docker:</li> </ul> <p><pre><code>sudo apt update &amp;&amp; sudo apt install -y docker.io\n</code></pre> * Pulando a imagem p\u00fablica:</p> <p><pre><code>docker pull luizdurand/api-fastapi-nuvem:latest\n</code></pre> * Executar container apontando para MySQL Router:</p> <pre><code>docker run -d \\\n  --name api_fastapi \\\n  -e DATABASE_URL=\"mysql+pymysql://cloud:cloud-n123@172.16.0.152:3306/p_cloud\" \\\n  -e SECRET_KEY=\"sua_chave_jwt_super_secreta\" \\\n  -p 8080:8080 \\\n  luizdurand/api-fastapi-nuvem:latest\n</code></pre>"},{"location":"projeto/main/#43-verificacao-do-status-juju","title":"4.3 Verifica\u00e7\u00e3o do Status Juju","text":"<ul> <li>Comando: <code>juju status mysql-innodb-cluster/1</code>.</li> <li>Resultado: <code>public-address: 172.16.0.152</code>.</li> <li>O <code>mysql-router/0</code> gerencia fail-over.</li> </ul>"},{"location":"projeto/main/#44-configuracao-do-load-balancer-lb-vm","title":"4.4 Configura\u00e7\u00e3o do Load Balancer (lb-vm)","text":"<ul> <li>Instalar Nginx:</li> </ul> <p><pre><code>sudo apt update &amp;&amp; sudo apt install -y nginx\n</code></pre> * Arquivo <code>/etc/nginx/sites-available/default</code>:</p> <p><pre><code>upstream apis {\n    server &lt;IP_API_VM1&gt;:8080;\n    server &lt;IP_API_VM2&gt;:8080;\n}\nserver {\n    listen 80;\n    location / {\n        proxy_pass http://apis;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n    }\n}\n</code></pre> * Testar e reiniciar:</p> <p><pre><code>sudo nginx -t &amp;&amp; sudo systemctl restart nginx\n</code></pre> * Associar Floating IP \u00e0 VM de LB para acesso externo: <code>http://&lt;FLOATING_IP&gt;/docs</code>.</p>"},{"location":"projeto/main/#5-tarefa-4-publicacao-da-imagem-no-docker-hub","title":"5. Tarefa 4 - Publica\u00e7\u00e3o da Imagem no Docker Hub","text":""},{"location":"projeto/main/#51-build-e-push","title":"5.1 Build e Push","text":"<pre><code>cd ~/Desktop/CLOUD/api_cloud/api\ndocker build -t luizdurand/api-fastapi-nuvem:latest .\ndocker login\ndocker push luizdurand/api-fastapi-nuvem:latest\n</code></pre>"},{"location":"projeto/main/#52-teste-em-nova-vm","title":"5.2 Teste em Nova VM","text":"<ol> <li>Em uma VM qualquer:</li> </ol> <p><pre><code>docker pull luizdurand/api-fastapi-nuvem:latest\ndocker run -d \\\n  --name api_fastapi \\\n  -e DATABASE_URL=\"mysql+pymysql://cloud:cloud-n123@db-externo:3306/p_cloud\" \\\n  -e SECRET_KEY=\"sua_chave_jwt_super_secreta\" \\\n  -p 8080:8080 \\\n  luizdurand/api-fastapi-nuvem:latest\n</code></pre> 2. Acessar: <code>http://&lt;IP_VM&gt;:8080/docs</code> e verificar conex\u00e3o com banco remoto.</p>"},{"location":"projeto/main/#6-testes-finais-e-conclusao","title":"6. Testes Finais e Conclus\u00e3o","text":"<ul> <li>Persist\u00eancia: ap\u00f3s reiniciar containers ou VMs, dados em <code>users</code> permanecem.</li> <li>Autentica\u00e7\u00e3o: fluxo de registro, login e consulta protegido por JWT funcionando.</li> <li>Balanceamento: Nginx distribui tr\u00e1fego entre <code>api-vm1</code> e <code>api-vm2</code>; falha de uma inst\u00e2ncia n\u00e3o interrompe servi\u00e7o.</li> <li>Alta Disponibilidade: InnoDB Cluster + mysql-router garantem continuidade em caso de falha de at\u00e9 uma unidade.</li> </ul> <p>O projeto demonstra a integra\u00e7\u00e3o completa entre c\u00f3digo, containers, reposit\u00f3rio Docker e infra em nuvem, garantindo escalabilidade, seguran\u00e7a e alta disponibilidade.</p>"},{"location":"projeto/main/#7-tarefa-5-deploy-na-aws-lightsail","title":"7. Tarefa 5 - Deploy na AWS Lightsail","text":"<p>Al\u00e9m das etapas anteriores, realizamos o deploy da API e do MySQL na AWS Lightsail. As principais configura\u00e7\u00f5es foram:</p>"},{"location":"projeto/main/#71-instancia-mysql-fastapi-db","title":"7.1 Inst\u00e2ncia MySQL (fastapi-db)","text":"<ul> <li>Provisionamento: Criamos uma inst\u00e2ncia Lightsail do tipo MySQL (8.0.42) com 1 GB RAM, 2 vCPUs e 40 GB SSD.</li> <li>Credenciais: usu\u00e1rio <code>admin</code> com senha vis\u00edvel nas configura\u00e7\u00f5es (e.g. <code>SenhaSegura123</code>).</li> <li>Endpoint P\u00fablico: Exemplo de endpoint: <code>ls-6f2e65f181730bb8f397f6fa2692ccbcd64f9e99.c90ug4gek4kp4.us-west-2.rds.amazonaws.com:3306</code>.</li> <li>Configura\u00e7\u00f5es de Rede: Modo p\u00fablico habilitado para permitir conex\u00f5es externas.</li> </ul>"},{"location":"projeto/main/#72-container-service-fastapi-service","title":"7.2 Container Service (fastapi-service)","text":"<ul> <li>Configura\u00e7\u00e3o do Servi\u00e7o de Container: Criamos um servi\u00e7o Lightsail do tipo Nano (512\u202fMB RAM, 0.25 vCPUs).</li> <li>Imagem Docker: <code>luizdurand/api-fastapi-nuvem:latest</code> carregada diretamente no Console do Lightsail.</li> <li> <p>Vari\u00e1veis de Ambiente: Definimos:</p> </li> <li> <p><code>DATABASE_URL</code>: <code>mysql+pymysql://admin:SenhaSegura123@ls-6f2e65f181730bb8f397f6fa2692ccbcd64f9e99.c90ug4gek4kp4.us-west-2.rds.amazonaws.com:3306/dbmaster</code></p> </li> <li><code>SECRET_KEY</code>: <code>troque_isto_por_uma_chave_secreta</code></li> <li>Portas Abertas: Porta 8080 exposta publicamente.</li> </ul> <p>Em seguida, habilitamos o servi\u00e7o e aguardamos o deploy concluir. A API passou a responder em um dom\u00ednio p\u00fablico fornecido pelo Lightsail, permitindo acesso externo via HTTP.</p> <p>Para visualizar a demonstra\u00e7\u00e3o da aplica\u00e7\u00e3o FastAPI em funcionamento ap\u00f3s o deploy na AWS Lightsail, assista ao v\u00eddeo a seguir: https://youtu.be/yFd4dmGgxxA</p> <ul> <li>Configura\u00e7\u00e3o do Servi\u00e7o de Container: Criamos um servi\u00e7o Lightsail do tipo Nano (512\u202fMB RAM, 0.25 vCPUs).</li> <li>Imagem Docker: <code>luizdurand/api-fastapi-nuvem:latest</code> carregada diretamente no Console do Lightsail.</li> <li> <p>Vari\u00e1veis de Ambiente: Definimos:</p> </li> <li> <p><code>DATABASE_URL</code>: <code>mysql+pymysql://admin:SenhaSegura123@ls-6f2e65f181730bb8f397f6fa2692ccbcd64f9e99.c90ug4gek4kp4.us-west-2.rds.amazonaws.com:3306/dbmaster</code></p> </li> <li><code>SECRET_KEY</code>: <code>troque_isto_por_uma_chave_secreta</code></li> <li>Portas Abertas: Porta 8080 exposta publicamente.</li> </ul> <p>Em seguida, habilitamos o servi\u00e7o e aguardamos o deploy concluir. A API passou a responder em um dom\u00ednio p\u00fablico fornecido pelo Lightsail, permitindo acesso externo via HTTP.</p>"},{"location":"projeto/main/#8-estimativa-de-custo-mensal","title":"8. Estimativa de Custo Mensal","text":"<p>Para estimar o custo mensal da infraestrutura na AWS Lightsail, utilizamos dois componentes principais:</p> <ol> <li> <p>Inst\u00e2ncia de Banco de Dados (fastapi-db)</p> </li> <li> <p>Escolhemos o plano Standard de $15 USD/m\u00eas, que inclui:</p> <ul> <li>1 GB RAM</li> <li>2 vCPUs</li> <li>40 GB SSD</li> <li>Este plano atende \u00e0 necessidade de armazenamento do banco MySQL e garante desempenho suficiente para opera\u00e7\u00f5es CRUD b\u00e1sicas.</li> </ul> </li> <li> <p>Servi\u00e7o de Container (fastapi-service)</p> </li> <li> <p>Selecionamos o plano Nano de $7 USD/m\u00eas, que inclui:</p> <ul> <li>512 MB RAM</li> <li>0.25 vCPUs</li> <li>Este plano \u00e9 adequado para hospedar a API reutilizando a imagem Docker j\u00e1 preparada, garantindo acesso p\u00fablico ao servi\u00e7o.</li> </ul> </li> </ol>"},{"location":"projeto/main/#calculo-total","title":"C\u00e1lculo Total","text":"<ul> <li>Banco de Dados: $15 USD/m\u00eas</li> <li>Servi\u00e7o de Container:  $7 USD/m\u00eas</li> </ul> <p>Total estimado: $22 USD/m\u00eas (\u2264 USD 50 conforme limite estabelecido)</p> <p>Este valor inclui apenas os recursos principais (banco e container). A AWS Lightsail n\u00e3o cobra por transfer\u00eancia de dados dentro da cota mensal de 500 GB; excedentes podem gerar custos adicionais baixos (ex.: $0.09 USD/GB extra). Assim, permanecemos confortavelmente dentro do or\u00e7amento.</p>"},{"location":"projeto/main/#9-diagrama-da-arquitetura","title":"9. Diagrama da Arquitetura","text":"<p>A seguir, apresentamos o diagrama da arquitetura final do projeto. Ele ilustra como o Client / User interage com o Lightsail Container Service (API Container) via HTTPS e como a API se comunica com o Lightsail Managed MySQL (Database) via SQL. Esse diagrama est\u00e1 localizado imediatamente abaixo deste texto.</p> <p></p>"},{"location":"roteiro1/main/","title":"Roteiro 1","text":""},{"location":"roteiro1/main/#objetivo","title":"Objetivo","text":"<p>Aqui vai o objetivo macro do roteiro. Por que estamos fazendo o que estamos fazendo?</p>"},{"location":"roteiro1/main/#infra","title":"Infra","text":"<p>Os pontos \"tarefas\" s\u00e3o os passos que devem ser seguidos para a realiza\u00e7\u00e3o do roteiro. Eles devem ser claros e objetivos. Com evid\u00eancias claras de que foram realizados.</p>"},{"location":"roteiro1/main/#tarefa-1","title":"Tarefa 1","text":"<p>Instalando o MAAS:</p> sudo snap install maas --channel=3.5/Stable <p></p> <p>Dashboard do MAAS</p> <p>Conforme ilustrado acima, a tela inicial do MAAS apresenta um dashboard com informa\u00e7\u00f5es sobre o estado atual dos servidores gerenciados. O dashboard \u00e9 composto por diversos pain\u00e9is, cada um exibindo informa\u00e7\u00f5es sobre um aspecto espec\u00edfico do ambiente gerenciado. Os pain\u00e9is podem ser configurados e personalizados de acordo com as necessidades do usu\u00e1rio.</p>"},{"location":"roteiro1/main/#tarefa-2","title":"Tarefa 2","text":""},{"location":"roteiro1/main/#app","title":"App","text":""},{"location":"roteiro1/main/#django-em-nuvem-bare-metal","title":"Django em Nuvem Bare-Metal","text":"<p>Postgres \u00e9 um servidor de banco de dados vers\u00e1til e de f\u00e1cil manejo. Muito usado em projetos Opensource e por isso vamos .</p> <p>Durante a configura\u00e7\u00e3o inicial da nossa nuvem MaaS, realizamos um deploy manual de uma aplica\u00e7\u00e3o simples em Django. Para garantir que o ambiente estivesse corretamente configurado, fizemos um pequeno ajuste no servidor DNS.</p> <p>Primeiramente, dentro da aba Subnets, acessamos a subnet <code>172.16.0.0/20</code> e editamos o Subnet Summary, substituindo o DNS pelo do Insper (<code>172.20.129.131</code>).</p>"},{"location":"roteiro1/main/#parte-1-banco-de-dados","title":"Parte 1: Banco de Dados","text":"<p>Primeiro Deploy</p> <p>Durante a configura\u00e7\u00e3o inicial da nossa nuvem MaaS, realizamos um deploy manual de uma aplica\u00e7\u00e3o simples em Django. Para garantir que o ambiente estivesse corretamente configurado, fizemos um pequeno ajuste no servidor DNS.</p> <p>Primeiramente, dentro da aba Subnets, acessamos a subnet <code>172.16.0.0/20</code> e editamos o Subnet Summary, substituindo o DNS pelo do Insper (<code>172.20.129.131</code>).</p> <p>Banco de Dados</p> <p>Como banco de dados, optamos pelo PostgreSQL, devido \u00e0 sua versatilidade e facilidade de manejo, sendo amplamente utilizado em projetos Open Source.</p> <p>Nosso primeiro passo foi acessar o Dashboard do MaaS e realizar o deploy do Ubuntu 22.04 no <code>server1</code>. Assim que a m\u00e1quina estava operacional, conectamos ao terminal via SSH e realizamos a instala\u00e7\u00e3o do PostgreSQL:</p> <pre><code>$ sudo apt update\n$ sudo apt install postgresql postgresql-contrib -y\n</code></pre> <p>Com o banco instalado, precis\u00e1vamos configurar um usu\u00e1rio para a aplica\u00e7\u00e3o. Para isso, acessamos o usu\u00e1rio <code>postgres</code>:</p> <pre><code>$ sudo su - postgres\n$ createuser -s cloud -W\n</code></pre> <p>Definimos a senha <code>cloud</code> e seguimos para a cria\u00e7\u00e3o do banco de dados:</p> <pre><code>$ createdb -O cloud tasks\n</code></pre> <p>Para garantir que o banco pudesse ser acessado remotamente dentro da rede, ajustamos os arquivos de configura\u00e7\u00e3o.</p> <p>No arquivo postgresql.conf, removemos o coment\u00e1rio da linha <code>listen_addresses</code> e a configuramos para aceitar conex\u00f5es externas:</p> <pre><code>$ nano /etc/postgresql/14/main/postgresql.conf\n</code></pre> <p>Modificamos a linha:</p> <pre><code>listen_addresses = '*'\n</code></pre> <p>Em seguida, editamos o arquivo pg_hba.conf para permitir conex\u00f5es de qualquer m\u00e1quina dentro da subnet do kit:</p> <pre><code>$ nano /etc/postgresql/14/main/pg_hba.conf\n</code></pre> <p>Adicionamos a seguinte linha:</p> <pre><code>host    all             all             172.16.0.0/20          trust\n</code></pre> <p>Ap\u00f3s essas configura\u00e7\u00f5es, sa\u00edmos do usu\u00e1rio <code>postgres</code> e liberamos a porta do banco no firewall:</p> <pre><code>$ sudo ufw allow 5432/tcp\n</code></pre> <p>Por fim, reiniciamos o servi\u00e7o para aplicar as altera\u00e7\u00f5es:</p> <pre><code>$ sudo systemctl restart postgresql\n</code></pre> <p>Essa configura\u00e7\u00e3o garantiu que o banco de dados estivesse pronto para ser utilizado pela aplica\u00e7\u00e3o Django em um ambiente bare-metal dentro da nossa nuvem MaaS.</p> <ol> <li>Funcionando e seu Status est\u00e1 como \"Ativo\" para o Sistema Operacional</li> </ol> <p>Usamos o comando: <pre><code>sudo systemctl status postgresql\n</code></pre> para verificar o status do  PostgreSQL  2. Acessivel na pr\u00f3pria maquina na qual ele foi implantado. Para verificar que esta acessivel na pr\u00f3pria maquina na qual ele foi implantado foi utilizado o comando: <pre><code>psql -U cloud -d tasks -h localhost\n</code></pre>  3. Acessivel a partir de uma conex\u00e3o vinda da m\u00e1quina MAIN. E para verificar que esta acessivel a partir de uma conex\u00e3o vinda da m\u00e1quina MAIN: <pre><code>telnet [IP do server1] 5432\n</code></pre></p> <p> 4. Em qual porta este servi\u00e7o est\u00e1 funcionando.</p> <p>Finalmente verificamos em qual porta o PostgreSQL est\u00e1 rodando com: <pre><code>sudo ss -tulnp | grep postgres\n</code></pre></p> <p></p>"},{"location":"roteiro1/main/#parte-2-implementacao-manual-da-aplicacao-django-e-banco-de-dados","title":"Parte 2: Implementa\u00e7\u00e3o Manual da Aplica\u00e7\u00e3o Django e Banco de Dados","text":"<p>Configura\u00e7\u00e3o do MaaS</p> <p>Para iniciar, acessei o MaaS e fiz login utilizando o CLI com o seguinte comando:</p> <pre><code>maas login [meu_usuario] http://172.16.0.3:5240/MAAS/\n</code></pre> <p>Em seguida, obtive o token no dashboard dentro das configura\u00e7\u00f5es do usu\u00e1rio. Com o token em m\u00e3os, solicitei a reserva de uma m\u00e1quina no MaaS:</p> <pre><code>maas [meu_usuario] machines allocate name=server2\n</code></pre> <p>A resposta retornou um JSON contendo v\u00e1rias informa\u00e7\u00f5es, entre elas o <code>system_id</code>, que anotei para uso posterior.</p> <p>Deploy da M\u00e1quina e Instala\u00e7\u00e3o da Aplica\u00e7\u00e3o</p> <p>Com o <code>system_id</code>, iniciei o deploy da m\u00e1quina via CLI:</p> <pre><code>maas [meu_usuario] machine deploy [system_id]\n</code></pre> <p>Assim que o deploy foi conclu\u00eddo, acessei a m\u00e1quina <code>server2</code> via SSH e clonei o reposit\u00f3rio do projeto Django:</p> <pre><code>git clone https://github.com/raulikeda/tasks.git\n</code></pre> <p>Naveguei at\u00e9 o diret\u00f3rio <code>tasks</code> e executei o script de instala\u00e7\u00e3o:</p> <pre><code>cd tasks\n./install.sh\n</code></pre> <p>Ap\u00f3s a instala\u00e7\u00e3o, reiniciei a m\u00e1quina e aguardei o processo de inicializa\u00e7\u00e3o.</p> <p>Teste do Servi\u00e7o</p> <p>Como server1 n\u00e3o estava registrado no DNS, adicionei-o manualmente ao arquivo <code>/etc/hosts</code> do server2: Apos a m\u00e1quina reiniciar adicionei manualmente o IP no <code>/etc/hosts</code></p> <pre><code>[IP_DO_SERVER1]  server1\n</code></pre> <p>Esta configura\u00e7\u00e3o permite que o server2 resolva o nome <code>server1</code> para o endere\u00e7o IP <code>192.168.1.100</code>, mesmo sem uma entrada correspondente no servidor DNS.</p> <p>Note</p> <p>Esta altera\u00e7\u00e3o \u00e9 local apenas para server2 e n\u00e3o afeta outros sistemas na rede.</p> <p>Para validar que a aplica\u00e7\u00e3o estava rodando corretamente, realizei um teste de acesso ao servi\u00e7o na porta 8080 diretamente do terminal do MaaS:</p> <pre><code>wget http://[IP_server2]:8080/admin/\n</code></pre> <p>Se o acesso fosse bem-sucedido, a interface de administra\u00e7\u00e3o do Django estaria funcionando corretamente.</p> <p>Exposi\u00e7\u00e3o do Servi\u00e7o via T\u00fanel SSH</p> <p>Para acessar a aplica\u00e7\u00e3o no navegador sem precisar configurar NAT no roteador, utilizei um t\u00fanel SSH. Primeiro, desconectei do SSH do MaaS e reconectei utilizando o seguinte comando:</p> <pre><code>ssh cloud@10.103.0.X -L 8001:[IP_server2]:8080\n</code></pre> <p>Esse comando criou um t\u00fanel, redirecionando o servi\u00e7o rodando na porta 8080 do <code>server2</code> para a porta 8001 no meu localhost. Certifiquei-me de que a porta 8001 n\u00e3o estava em uso antes de rodar o comando.</p> <p>Ent\u00e3o, acessei o Django Admin pelo navegador no seguinte endere\u00e7o:</p> <pre><code>http://localhost:8001/admin/\n</code></pre> <p>Fiz login com as credenciais padr\u00e3o:</p> <ul> <li>Usu\u00e1rio: cloud</li> <li>Senha: cloud</li> </ul> <p>Tarefa 2</p> <p>1.Dashboard do MAAS com as m\u00e1quinas.  2. Da aba images, com as imagens sincronizadas.  3. Aba de cada maquina(5x) mostrando os testes de hardware e commissioning com Status \"OK\"  </p>"},{"location":"roteiro1/main/#parte-3-status-dos-servidores","title":"Parte 3: Status dos servidores","text":"<p>Tarefa-3</p> <p>Prints Comprovando a Configura\u00e7\u00e3o</p> <ul> <li>Capturamos um print da tela do Dashboard do MAAS, mostrando as duas m\u00e1quinas configuradas com seus respectivos IPs. </li> <li>Registramos tamb\u00e9m um print da aplica\u00e7\u00e3o Django em execu\u00e7\u00e3o, comprovando a conex\u00e3o com o servidor. </li> <li>Explique como foi feita a implementacao manual da aplicacao Django e banco de dados.</li> </ul> <p>Implementa\u00e7\u00e3o Manual da Aplica\u00e7\u00e3o Django e Banco de Dados</p> <p>Inicialmente, configuramos uma \u00fanica aplica\u00e7\u00e3o Django. No entanto, percebemos a necessidade de expandir para dois servidores de aplica\u00e7\u00e3o (server2 e server3), ambos conectados a um \u00fanico banco de dados hospedado no server1. Essa estrat\u00e9gia foi adotada por dois motivos principais:</p> <ol> <li>Alta Disponibilidade: Caso um dos n\u00f3s falhe, o outro continua operacional, garantindo acesso cont\u00ednuo aos usu\u00e1rios.</li> <li>Balanceamento de Carga: Distribu\u00edmos as requisi\u00e7\u00f5es entre os servidores, melhorando a escalabilidade e o desempenho da aplica\u00e7\u00e3o.</li> </ol> <p>Para otimizar esse processo, decidimos automatizar a instala\u00e7\u00e3o e configura\u00e7\u00e3o usando o Ansible. Essa ferramenta facilitou a replica\u00e7\u00e3o do ambiente em m\u00faltiplos servidores, garantindo consist\u00eancia e reduzindo erros manuais.</p>"},{"location":"roteiro1/main/#parte-4-utilizando-o-ansible-deploy-automatizado-de-aplicacao","title":"Parte 4: Utilizando o Ansible - Deploy Automatizado de Aplica\u00e7\u00e3o","text":"<p>Configura\u00e7\u00e3o do Server3 e Deploy com Ansible</p> <p>Ap\u00f3s  para o maas via cli, igual feito anteriormente para o server2, realizamos a instala\u00e7\u00e3o e configura\u00e7\u00e3o do Ansible no servidor principal (main). Utilizamos o seguinte procedimento:</p> <ol> <li>Instalamos o Ansible:    <pre><code>sudo apt install ansible\n</code></pre></li> <li>Baixamos o playbook de instala\u00e7\u00e3o da aplica\u00e7\u00e3o Django:    <pre><code>wget https://raw.githubusercontent.com/raulikeda/tasks/master/tasks-install-playbook.yaml\n</code></pre></li> <li>Executamos o playbook para provisionar o server3 automaticamente:    <pre><code>ansible-playbook tasks-install-playbook.yaml --extra-vars server=[IP server3]\n</code></pre></li> </ol> <p>O Ansible garantiu que todos os procedimentos fossem executados de forma idempotente, ou seja, pod\u00edamos repetir o processo sem impactar negativamente o ambiente. Al\u00e9m disso, possibilitou a configura\u00e7\u00e3o simult\u00e2nea de m\u00faltiplos servidores, facilitando a expans\u00e3o da infraestrutura.</p> <p>Tarefa 4</p> <ol> <li>Tela do Dashboard do MAAS com as 3 Maquinas e seus respectivos IPs. </li> <li>Aplicacao Django, provando que voce est\u00e1 conectado ao server2  </li> <li>Aplicacao Django, provando que voce est\u00e1 conectado ao server3  </li> <li>Diferenca entre instalar manualmente a aplicacao Django e utilizando o Ansible.</li> </ol> <p>Com essa abordagem, conseguimos implementar uma solu\u00e7\u00e3o escal\u00e1vel e confi\u00e1vel para nossa aplica\u00e7\u00e3o Django. A automa\u00e7\u00e3o proporcionada pelo Ansible simplificou o gerenciamento dos servidores e garantiu que a aplica\u00e7\u00e3o estivesse sempre dispon\u00edvel e distribu\u00edda adequadamente.</p>"},{"location":"roteiro1/main/#parte-4-balanceamento-de-carga-com-nginx","title":"Parte 4: Balanceamento de Carga com NGINX","text":"<p>Nossa Implementa\u00e7\u00e3o com NGINX</p> <p>Instala\u00e7\u00e3o do NGINX</p> <p>Primeiramentea foi feito o deploy e instala\u00e7\u00e3o do NGINX no servidor <code>server4</code> para atuar como nosso balanceador de carga:</p> <pre><code>sudo apt update\nsudo apt install nginx -y\n</code></pre> <p>Configura\u00e7\u00e3o do M\u00f3dulo Upstream</p> <p>Para configurar o balanceamento round robin, utilizamos o m\u00f3dulo upstream do NGINX. Editamos o arquivo de configura\u00e7\u00e3o do site padr\u00e3o:</p> <pre><code>sudo nano /etc/nginx/sites-available/default\n</code></pre> <p>Adicionamos a configura\u00e7\u00e3o do balanceamento de carga da seguinte forma:</p> <pre><code>upstream backend {\n    server [IP_DO_SERVER2] :8080;\n    server [IP_DO_SERVER3] :8080;\n}\n\n\nserver {\n    location / {\n        proxy_pass http://backend;\n    }\n}\n</code></pre> <p>Ap\u00f3s salvar as altera\u00e7\u00f5es, reiniciamos o servi\u00e7o do NGINX:</p> <p><pre><code>sudo service nginx restart\n</code></pre> Tarefa 5</p> <ol> <li> <p>De um print da tela do Dashboard do MAAS com as 4 Maquinas e seus respectivos IPs.  Personaliza\u00e7\u00e3o das Aplica\u00e7\u00f5es Django</p> </li> <li> <p>Alteramos o conte\u00fado da mensagem contida na fun\u00e7\u00e3o <code>index</code> do arquivo <code>tasks/views.py</code> de cada server para distinguir ambos os servers. </p> </li> </ol> <p>Para podermos identificar facilmente qual servidor estava respondendo a cada requisi\u00e7\u00e3o, modificamos o arquivo <code>tasks/views.py</code> em cada inst\u00e2ncia Django, alterando a mensagem de resposta para algo \u00fanico em cada servidor:</p> <p><pre><code># Em server1\nfrom django.shortcuts import render\nfrom django.http import HttpResponse\n\ndef index(request):\n    return HttpResponse(\"Hello, world. You're at server 2.\")\n</code></pre> </p> <p><pre><code># Em server2\nfrom django.shortcuts import render\nfrom django.http import HttpResponse\n\ndef index(request):\n    return HttpResponse(\"Hello, world. You're at server 2.\")\n</code></pre> </p> <ol> <li> <p>Fa\u00e7a um <code>GET request</code> para o path que voce criou em urls.py para o Nginx e tire 2 prints das respostas de cada request, provando que voce est\u00e1 conectado ao server 4, que \u00e9 o Proxy Reverso e que ele bate cada vez em um server diferente server2 e server3.</p> </li> <li> <p>As respostas de cada request do server 4, mostram que cada vez que um <code>GET request</code> \u00e9 feito ele bate cada vez em um server diferente server2 e server3.  </p> </li> </ol>"},{"location":"roteiro2/main/","title":"Relat\u00f3rio - Roteiro 2: Gerenciamento de Aplica\u00e7\u00f5es com Juju","text":""},{"location":"roteiro2/main/#objetivo","title":"Objetivo","text":"<p>O objetivo deste roteiro \u00e9 explorar o orquestrador Juju para gerenciar aplica\u00e7\u00f5es em ambientes de nuvem. Atrav\u00e9s do Juju, buscamos:</p> <ol> <li>Simplificar o deploy e gerenciamento de aplica\u00e7\u00f5es complexas  </li> <li>Implementar solu\u00e7\u00f5es de monitoramento com Prometheus e Grafana  </li> <li>Gerenciar relacionamentos entre servi\u00e7os de forma declarativa  </li> <li>Automatizar a configura\u00e7\u00e3o de ambientes escal\u00e1veis  </li> </ol>"},{"location":"roteiro2/main/#infra","title":"Infra","text":""},{"location":"roteiro2/main/#configuracao-inicial-do-ambiente-juju","title":"Configura\u00e7\u00e3o Inicial do Ambiente Juju","text":"<ol> <li>Instala\u00e7\u00e3o do Juju: <pre><code>sudo snap install juju --channel 3.6\n</code></pre> Depois, verificamos que o Juju reconhece o MaaS como provedor de recursos com: <pre><code>juju clouds\n</code></pre></li> </ol> <p>Como o MaaS n\u00e3o apareceu na lista de clouds do Juju, adicionamos o manualmente usando um arquivo YAML.</p> <p><pre><code>nano maas-cloud.yaml\n</code></pre> <pre><code>clouds:\n  my-maas:\n    type: maas\n    auth-types: [oauth1]\n    endpoint: http://&lt;SEU_IP_MAAS&gt;:5240/MAAS\n</code></pre> <pre><code>juju add-cloud --client my-maas maas-cloud.yaml\n</code></pre></p> <p>Adicionamos as credenciais MAAS para que o Juju possa interagir com a nova cloud adicionada. <pre><code>credentials:\n  maas-one:\n    anyuser:\n      auth-type: oauth1\n      maas-oauth: &lt;API KEY&gt;\n</code></pre> <pre><code>juju add-credential --client -f maas-creds.yaml maas-one\n</code></pre></p>"},{"location":"roteiro2/main/#criacao-do-controlador","title":"Cria\u00e7\u00e3o do Controlador","text":"<p>Para criar o controlador no <code>server1</code>, primeiro colocamos a tag \"juju\" na m\u00e1quina pelo painel do MAAS. Depois, executamos:</p> <pre><code>juju bootstrap --bootstrap-series=jammy --constraints tags=juju maas-one maas-controller\n</code></pre> <p>Esse processo pode demorar, pois o Juju precisa provisionar uma m\u00e1quina, instalar pacotes e configurar servi\u00e7os. Para acompanhar o progresso, utilizamos e deixaremos esse terminal aberto daqui em diante:</p> <pre><code>watch -n 1 --color 'juju status --color'\n</code></pre>"},{"location":"roteiro2/main/#app","title":"App","text":"<p><pre><code>juju add-model --config default-series=jammy openstack\njuju switch controller\njuju deploy juju-dashboard --to lxd:0\njuju intergrate juju-dashboard controller\n</code></pre> Cria o modelo openstack com a s\u00e9rie padr\u00e3o jammy. Troca para o modelo controller. Implanta o painel do Juju (juju-dashboard) em um container da m\u00e1quina 0. Integra o painel ao controlador para gerenciar os modelos via interface web.</p> <p><pre><code>juju enable-dashboard\njuju dashboard\n</code></pre> Ativa a interface web do Juju. Exibe o link de acesso e credenciais para login.</p> <p><pre><code>mkdir -p /home/cloud/charms\ncd /home/cloud/charms\n</code></pre> Cria e acessa o diret\u00f3rio onde os charms ser\u00e3o armazenados.</p> <p><pre><code>juju download grafana\njuju download prometheus2\n</code></pre> Baixa os charms Grafana e Prometheus2 localmente para futura implanta\u00e7\u00e3o.</p> <pre><code>juju switch openstack\njuju deploy ./prometheus2_r60.charm\njuju deploy grafana\n</code></pre> <p>Troca para o modelo openstack. Implanta o Prometheus2 a partir do arquivo .charm baixado. Implanta o Grafana diretamente do reposit\u00f3rio de charms.</p> <p><pre><code>juju intergrate grafana:grafana-source prometheus2:grafana-source\n</code></pre> Conecta Grafana ao Prometheus, permitindo uso das m\u00e9tricas como fonte de dados.</p> <p><pre><code>juju expose grafana\njuju expose prometheus2\n</code></pre> Libera acesso externo aos servi\u00e7os via rede.</p> <p>Acessamos o Grafana via navegador:</p> <p>Obtemos o IP da m\u00e1quina do grafana com juju status</p> <p>Acessamos http://:3000 e fizemos login com a senha resultante dessa linha <p><pre><code>juju run grafana/0 get-admin-password\n</code></pre> Fa\u00e7a login e configure seu dashboard!</p>"},{"location":"roteiro2/main/#tarefas","title":"Tarefas","text":""},{"location":"roteiro3/main/","title":"Roteiro 3 - OpenStack","text":""},{"location":"roteiro3/main/#objetivo","title":"Objetivo","text":"<p>Este roteiro tem como objetivo configurar e utilizar o ambiente OpenStack para integra\u00e7\u00e3o com Docker e Kubernetes, possibilitando gerenciamento eficiente de cont\u00eaineres e garantindo escalabilidade e alta disponibilidade dos servi\u00e7os.</p>"},{"location":"roteiro3/main/#tarefa-1","title":"Tarefa 1","text":""},{"location":"roteiro3/main/#status-juju","title":"Status JUJU","text":"<p>O JUJU foi utilizado para gerenciar as m\u00e1quinas f\u00edsicas e virtuais. A imagem abaixo apresenta o status das aplica\u00e7\u00f5es e servi\u00e7os gerenciados pelo JUJU.</p> <p></p>"},{"location":"roteiro3/main/#dashboard-do-maas-com-maquinas","title":"Dashboard do MAAS com m\u00e1quinas","text":"<p>O MAAS (Metal as a Service) mostra as m\u00e1quinas f\u00edsicas e seu status, indicando quais est\u00e3o dispon\u00edveis para utiliza\u00e7\u00e3o.</p> <p></p>"},{"location":"roteiro3/main/#aba-compute-overview-no-openstack","title":"Aba Compute Overview no OpenStack","text":"<p>Nesta aba temos uma vis\u00e3o geral do ambiente computacional do OpenStack, incluindo recursos utilizados e dispon\u00edveis.</p> <p></p>"},{"location":"roteiro3/main/#aba-compute-instances-no-openstack","title":"Aba Compute Instances no OpenStack","text":"<p>Aqui s\u00e3o apresentadas as inst\u00e2ncias virtuais ativas no ambiente, incluindo detalhes sobre recursos alocados e status.</p> <p></p>"},{"location":"roteiro3/main/#aba-network-topology-no-openstack","title":"Aba Network Topology no OpenStack","text":"<p>Esta aba ilustra a topologia da rede criada no OpenStack, evidenciando conex\u00f5es, redes e inst\u00e2ncias.</p> <p></p>"},{"location":"roteiro3/main/#tarefa-2","title":"Tarefa 2","text":""},{"location":"roteiro3/main/#dashboard-do-maas-com-as-maquinas","title":"Dashboard do MAAS com as m\u00e1quinas","text":"<p>Ap\u00f3s modifica\u00e7\u00f5es, o MAAS mostra as m\u00e1quinas f\u00edsicas com ajustes nas aloca\u00e7\u00f5es.</p> <p></p>"},{"location":"roteiro3/main/#aba-compute-overview-no-openstack_1","title":"Aba Compute Overview no OpenStack","text":"<p>Esta aba atualizada do OpenStack permite observar altera\u00e7\u00f5es no uso de recursos computacionais.</p> <p></p>"},{"location":"roteiro3/main/#aba-compute-instances-no-openstack_1","title":"Aba Compute Instances no OpenStack","text":"<p>A aba mostra novas inst\u00e2ncias ou altera\u00e7\u00f5es em inst\u00e2ncias anteriores.</p> <p></p>"},{"location":"roteiro3/main/#aba-network-topology-no-openstack_1","title":"Aba Network Topology no OpenStack","text":"<p>A nova topologia de rede reflete altera\u00e7\u00f5es feitas no ambiente.</p> <p></p> <p>Diferen\u00e7as observadas entre Tarefa 1 e Tarefa 2: - Foram identificadas diferen\u00e7as principalmente na configura\u00e7\u00e3o das inst\u00e2ncias, utiliza\u00e7\u00e3o de recursos e poss\u00edveis altera\u00e7\u00f5es na rede ap\u00f3s configura\u00e7\u00e3o adicional.</p> <p>Recursos criados e modificados: - Inst\u00e2ncias virtuais, recursos computacionais e ajustes na topologia de rede para suportar maior carga ou diferentes requisitos.</p>"},{"location":"roteiro3/main/#tarefa-3","title":"Tarefa 3","text":""},{"location":"roteiro3/main/#arquitetura-de-rede-com-sub-redes","title":"Arquitetura de rede com sub-redes","text":"<p>A imagem a seguir representa toda a arquitetura de rede configurada, desde o acesso do usu\u00e1rio no Insper at\u00e9 a inst\u00e2ncia final alocada no OpenStack. Ela mostra claramente a transi\u00e7\u00e3o entre redes (Rede Insper, VPN, Internet), a entrada pelo roteador do OpenStack e a separa\u00e7\u00e3o entre rede externa e interna. Nessa rede interna, temos todas as VMs alocadas, como Load Balancer, inst\u00e2ncias de aplica\u00e7\u00e3o, banco de dados e cliente. As sub-redes utilizadas tamb\u00e9m est\u00e3o indicadas explicitamente para melhor entendimento da estrutura l\u00f3gica.</p> <p></p>"},{"location":"roteiro3/main/#tarefa-4","title":"Tarefa 4","text":""},{"location":"roteiro3/main/#relatorio-dos-passos-utilizados","title":"Relat\u00f3rio dos passos utilizados","text":"<p>Nesta tarefa foram executados os seguintes passos detalhados:</p> <ol> <li>Instala\u00e7\u00e3o e configura\u00e7\u00e3o inicial do OpenStack, incluindo Octavia para balanceamento de carga.</li> <li>Cria\u00e7\u00e3o da arquitetura de rede necess\u00e1ria para o ambiente.</li> <li>Defini\u00e7\u00e3o e configura\u00e7\u00e3o de inst\u00e2ncias virtuais (VMs) com nomes e IPs espec\u00edficos.</li> <li>Implementa\u00e7\u00e3o de uma aplica\u00e7\u00e3o FastAPI conectada atrav\u00e9s de Nginx/LoadBalancer.</li> <li>Aloca\u00e7\u00e3o e gerenciamento das inst\u00e2ncias em m\u00e1quinas f\u00edsicas diferentes via OpenStack.</li> </ol>"},{"location":"roteiro3/main/#arquitetura-de-rede","title":"Arquitetura de rede","text":"<p>Abaixo a imagem detalha a arquitetura de rede configurada no ambiente OpenStack, evidenciando conex\u00f5es entre servi\u00e7os, redes e inst\u00e2ncias.</p> <p></p>"},{"location":"roteiro3/main/#lista-de-vms-com-nomes-e-ips-alocados","title":"Lista de VMs com nomes e IPs alocados","text":"<p>Nesta imagem \u00e9 poss\u00edvel verificar todas as VMs configuradas, incluindo seus respectivos nomes e IPs.</p> <p></p>"},{"location":"roteiro3/main/#dashboard-do-fastapi-conectado-via-maquina-nginxlb","title":"Dashboard do FastAPI conectado via m\u00e1quina Nginx/LB","text":"<p>O dashboard do servi\u00e7o FastAPI mostra sua disponibilidade atrav\u00e9s da interface configurada pelo Nginx Load Balancer.</p> <p></p>"},{"location":"roteiro3/main/#instancias-e-suas-alocacoes-nos-servidores-maquinas-fisicas","title":"Inst\u00e2ncias e suas aloca\u00e7\u00f5es nos servidores (m\u00e1quinas f\u00edsicas)","text":"<p>As imagens seguintes ilustram em quais m\u00e1quinas f\u00edsicas cada inst\u00e2ncia virtual foi alocada:</p>"},{"location":"roteiro4/main/","title":"Roteiro 4 - OpenStack e Kubernetes: Checkpoint por Aluno","text":""},{"location":"roteiro4/main/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Neste roteiro, avan\u00e7amos na consolida\u00e7\u00e3o do dom\u00ednio do OpenStack por meio de um checkpoint individual. A proposta \u00e9 garantir que cada aluno seja capaz de navegar pelas diferentes abas da plataforma, compreendendo a l\u00f3gica de projetos, usu\u00e1rios, aloca\u00e7\u00e3o de inst\u00e2ncias, al\u00e9m de analisar a topologia de rede resultante da sua configura\u00e7\u00e3o.</p> <p>A verifica\u00e7\u00e3o parte de um conjunto m\u00ednimo de prints que comprovam o uso adequado da interface de gerenciamento e a correta associa\u00e7\u00e3o de recursos.</p>"},{"location":"roteiro4/main/#tarefa-1-a-6-verificacoes-no-painel-openstack","title":"Tarefa 1 a 6 - Verifica\u00e7\u00f5es no Painel OpenStack","text":"<p>Abaixo est\u00e3o os prints que comprovam o estado atual da minha infraestrutura dentro do OpenStack, conforme solicitado:</p>"},{"location":"roteiro4/main/#identity-aba-projects","title":"Identity - Aba \"Projects\"","text":"<p>Exibe os projetos associados ao ambiente OpenStack. Cada projeto agrupa recursos e usu\u00e1rios, permitindo o isolamento de ambientes.</p> <p></p>"},{"location":"roteiro4/main/#identity-aba-users","title":"Identity - Aba \"Users\"","text":"<p>Mostra os usu\u00e1rios cadastrados, suas fun\u00e7\u00f5es e permiss\u00f5es dentro dos projetos. Cada usu\u00e1rio tem pap\u00e9is distintos e pode interagir com recursos conforme suas permiss\u00f5es.</p> <p></p>"},{"location":"roteiro4/main/#compute-aba-overview","title":"Compute - Aba \"Overview\"","text":"<p>Painel de vis\u00e3o geral com consumo de recursos, como inst\u00e2ncias, vCPUs, RAM e armazenamento. \u00datil para controle de capacidade.</p> <p></p>"},{"location":"roteiro4/main/#compute-aba-instances","title":"Compute - Aba \"Instances\"","text":"<p>Lista detalhada de inst\u00e2ncias ativas, incluindo nome, status, IP e recursos alocados.</p> <p></p>"},{"location":"roteiro4/main/#network-aba-topology-visao-1","title":"Network - Aba \"Topology\" (Vis\u00e3o 1)","text":"<p>Apresenta visualmente a topologia de rede criada, com destaque para sub-redes, roteadores e inst\u00e2ncias conectadas.</p> <p></p>"},{"location":"roteiro4/main/#network-aba-topology-visao-2","title":"Network - Aba \"Topology\" (Vis\u00e3o 2)","text":"<p>Outra vis\u00e3o complementar da mesma topologia, demonstrando a comunica\u00e7\u00e3o entre componentes virtuais.</p> <p></p>"},{"location":"roteiro4/main/#topologia-detalhada-da-aplicacao","title":"Topologia Detalhada da Aplica\u00e7\u00e3o","text":"<p>Visualiza\u00e7\u00e3o da arquitetura montada com Kubernetes e servi\u00e7os integrados, incluindo Load Balancer, banco de dados, APIs e cliente.</p> <p> </p>"},{"location":"roteiro4/main/#terraform-automacao-da-infraestrutura-como-codigo","title":"Terraform: Automa\u00e7\u00e3o da Infraestrutura como C\u00f3digo","text":"<p>Neste roteiro, foi fundamental a utiliza\u00e7\u00e3o do Terraform como ferramenta de Infraestrutura como C\u00f3digo (IaC) para provisionar e gerenciar os recursos no OpenStack de forma automatizada e reprodut\u00edvel. O Terraform permite definir a infraestrutura desejada por meio de arquivos de configura\u00e7\u00e3o (<code>.tf</code>), garantindo consist\u00eancia e versionamento das altera\u00e7\u00f5es.</p> <p>Etapas realizadas:</p> <ol> <li> <p>Defini\u00e7\u00e3o da infraestrutura: Foram criados arquivos <code>.tf</code> especificando redes, sub-redes, roteadores e inst\u00e2ncias, adaptados \u00e0s necessidades de cada aluno.</p> </li> <li> <p>Inicializa\u00e7\u00e3o do Terraform: Com o comando <code>terraform init</code>, o ambiente foi preparado, baixando os plugins necess\u00e1rios para o provedor OpenStack.</p> </li> <li> <p>Planejamento das altera\u00e7\u00f5es: Utilizando <code>terraform plan</code>, foi poss\u00edvel visualizar as a\u00e7\u00f5es que o Terraform executaria, permitindo uma revis\u00e3o antes da aplica\u00e7\u00e3o.</p> </li> <li> <p>Aplica\u00e7\u00e3o das configura\u00e7\u00f5es: Por fim, <code>terraform apply</code> foi executado para criar ou atualizar os recursos conforme definido nos arquivos de configura\u00e7\u00e3o.</p> </li> </ol> <p>Essa abordagem proporcionou uma gest\u00e3o eficiente da infraestrutura, facilitando a replica\u00e7\u00e3o do ambiente e minimizando erros manuais. Al\u00e9m disso, o uso do Terraform alinhou-se \u00e0s pr\u00e1ticas modernas de DevOps, promovendo agilidade e controle no gerenciamento dos recursos em nuvem.</p>"},{"location":"roteiro4/main/#plano-de-disaster-recovery-e-sla","title":"Plano de Disaster Recovery e SLA","text":"<p>Diante da responsabilidade de liderar a implanta\u00e7\u00e3o de um sistema cr\u00edtico, distribu\u00eddo em v\u00e1rias regi\u00f5es do Brasil e com alto grau de confidencialidade, a escolha da infraestrutura se d\u00e1 pela ado\u00e7\u00e3o de uma nuvem privada (Private Cloud). Isso garante controle direto sobre os dados sens\u00edveis, menor exposi\u00e7\u00e3o p\u00fablica e permite customiza\u00e7\u00f5es espec\u00edficas de seguran\u00e7a e rede.</p> <p>Para sustentar a opera\u00e7\u00e3o cont\u00ednua e minimizar falhas, a presen\u00e7a de um time DevOps \u00e9 essencial. Ele garante a automa\u00e7\u00e3o de processos, integra\u00e7\u00e3o cont\u00ednua, entrega \u00e1gil de melhorias e respostas r\u00e1pidas a incidentes. O time atua de forma colaborativa entre desenvolvimento e opera\u00e7\u00f5es, otimizando o ciclo de vida das aplica\u00e7\u00f5es.</p> <p>Pensando em resili\u00eancia e continuidade, elaboramos um plano de Disaster Recovery (DR) e Alta Disponibilidade (HA) que prev\u00ea os seguintes pilares:</p> <ul> <li> <p>Mapeamento de amea\u00e7as: Foram identificadas amea\u00e7as como falhas de hardware, erros humanos, ataques cibern\u00e9ticos (DDoS, ransomware), indisponibilidade de links de rede e falhas de energia nos datacenters. A an\u00e1lise permitiu priorizar os riscos de maior impacto e frequ\u00eancia.</p> </li> <li> <p>Plano de recupera\u00e7\u00e3o: A estrat\u00e9gia prev\u00ea snapshots autom\u00e1ticos e regulares das VMs e volumes cr\u00edticos, failover automatizado entre inst\u00e2ncias replicadas, e reimplanta\u00e7\u00f5es r\u00e1pidas via scripts de infraestrutura como c\u00f3digo (IaC). Tamb\u00e9m s\u00e3o utilizados testes peri\u00f3dicos de recupera\u00e7\u00e3o para garantir a efic\u00e1cia dos procedimentos.</p> </li> <li> <p>Pol\u00edtica de backup: A equipe estabelece backups di\u00e1rios incrementais e semanais completos, com reten\u00e7\u00e3o de no m\u00ednimo 30 dias. Os dados s\u00e3o replicados em zonas de disponibilidade diferentes, com verifica\u00e7\u00e3o automatizada da integridade.</p> </li> <li> <p>Alta disponibilidade: Load Balancers e inst\u00e2ncias em clusters replicados garantem balanceamento de carga e toler\u00e2ncia a falhas. O uso de Kubernetes permite a redistribui\u00e7\u00e3o autom\u00e1tica de pods em caso de falhas f\u00edsicas, enquanto servi\u00e7os cr\u00edticos s\u00e3o monitorados por probes de sa\u00fade.</p> </li> </ul> <p>Este plano garante que a infraestrutura possa resistir a incidentes, restaurar opera\u00e7\u00f5es em minutos e cumprir os SLAs estabelecidos com seguran\u00e7a e robustez.</p>"},{"location":"roteiro4/main/#conclusao","title":"Conclus\u00e3o","text":"<p>Este checkpoint refor\u00e7a a import\u00e2ncia do dom\u00ednio individual da plataforma OpenStack, especialmente na gest\u00e3o de usu\u00e1rios, projetos e inst\u00e2ncias. Os prints apresentados atestam a execu\u00e7\u00e3o correta das tarefas propostas, bem como a configura\u00e7\u00e3o funcional da infraestrutura de rede. O dom\u00ednio dessas opera\u00e7\u00f5es b\u00e1sicas garante uma base s\u00f3lida para a implanta\u00e7\u00e3o e manuten\u00e7\u00e3o de solu\u00e7\u00f5es em nuvem mais complexas.</p>"}]}