{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computa\u00e7\u00e3o em Nuvem","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#kit-n","title":"KIT-N","text":"<p>Luka Figueiredo</p> <p>Luiz Durand</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 21/03/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"roteiro1/main/","title":"Roteiro 1","text":""},{"location":"roteiro1/main/#objetivo","title":"Objetivo","text":"<p>Aqui vai o objetivo macro do roteiro. Por que estamos fazendo o que estamos fazendo?</p>"},{"location":"roteiro1/main/#infra","title":"Infra","text":"<p>Os pontos \"tarefas\" s\u00e3o os passos que devem ser seguidos para a realiza\u00e7\u00e3o do roteiro. Eles devem ser claros e objetivos. Com evid\u00eancias claras de que foram realizados.</p>"},{"location":"roteiro1/main/#tarefa-1","title":"Tarefa 1","text":"<p>Instalando o MAAS:</p> sudo snap install maas --channel=3.5/Stable <p></p> <p>Dashboard do MAAS</p> <p>Conforme ilustrado acima, a tela inicial do MAAS apresenta um dashboard com informa\u00e7\u00f5es sobre o estado atual dos servidores gerenciados. O dashboard \u00e9 composto por diversos pain\u00e9is, cada um exibindo informa\u00e7\u00f5es sobre um aspecto espec\u00edfico do ambiente gerenciado. Os pain\u00e9is podem ser configurados e personalizados de acordo com as necessidades do usu\u00e1rio.</p>"},{"location":"roteiro1/main/#tarefa-2","title":"Tarefa 2","text":""},{"location":"roteiro1/main/#app","title":"App","text":""},{"location":"roteiro1/main/#django-em-nuvem-bare-metal","title":"Django em Nuvem Bare-Metal","text":"<p>Postgres \u00e9 um servidor de banco de dados vers\u00e1til e de f\u00e1cil manejo. Muito usado em projetos Opensource e por isso vamos .</p> <p>Durante a configura\u00e7\u00e3o inicial da nossa nuvem MaaS, realizamos um deploy manual de uma aplica\u00e7\u00e3o simples em Django. Para garantir que o ambiente estivesse corretamente configurado, fizemos um pequeno ajuste no servidor DNS.</p> <p>Primeiramente, dentro da aba Subnets, acessamos a subnet <code>172.16.0.0/20</code> e editamos o Subnet Summary, substituindo o DNS pelo do Insper (<code>172.20.129.131</code>).</p>"},{"location":"roteiro1/main/#parte-1-banco-de-dados","title":"Parte 1: Banco de Dados","text":"<p>Primeiro Deploy</p> <p>Durante a configura\u00e7\u00e3o inicial da nossa nuvem MaaS, realizamos um deploy manual de uma aplica\u00e7\u00e3o simples em Django. Para garantir que o ambiente estivesse corretamente configurado, fizemos um pequeno ajuste no servidor DNS.</p> <p>Primeiramente, dentro da aba Subnets, acessamos a subnet <code>172.16.0.0/20</code> e editamos o Subnet Summary, substituindo o DNS pelo do Insper (<code>172.20.129.131</code>).</p> <p>Banco de Dados</p> <p>Como banco de dados, optamos pelo PostgreSQL, devido \u00e0 sua versatilidade e facilidade de manejo, sendo amplamente utilizado em projetos Open Source.</p> <p>Nosso primeiro passo foi acessar o Dashboard do MaaS e realizar o deploy do Ubuntu 22.04 no <code>server1</code>. Assim que a m\u00e1quina estava operacional, conectamos ao terminal via SSH e realizamos a instala\u00e7\u00e3o do PostgreSQL:</p> <pre><code>$ sudo apt update\n$ sudo apt install postgresql postgresql-contrib -y\n</code></pre> <p>Com o banco instalado, precis\u00e1vamos configurar um usu\u00e1rio para a aplica\u00e7\u00e3o. Para isso, acessamos o usu\u00e1rio <code>postgres</code>:</p> <pre><code>$ sudo su - postgres\n$ createuser -s cloud -W\n</code></pre> <p>Definimos a senha <code>cloud</code> e seguimos para a cria\u00e7\u00e3o do banco de dados:</p> <pre><code>$ createdb -O cloud tasks\n</code></pre> <p>Para garantir que o banco pudesse ser acessado remotamente dentro da rede, ajustamos os arquivos de configura\u00e7\u00e3o.</p> <p>No arquivo postgresql.conf, removemos o coment\u00e1rio da linha <code>listen_addresses</code> e a configuramos para aceitar conex\u00f5es externas:</p> <pre><code>$ nano /etc/postgresql/14/main/postgresql.conf\n</code></pre> <p>Modificamos a linha:</p> <pre><code>listen_addresses = '*'\n</code></pre> <p>Em seguida, editamos o arquivo pg_hba.conf para permitir conex\u00f5es de qualquer m\u00e1quina dentro da subnet do kit:</p> <pre><code>$ nano /etc/postgresql/14/main/pg_hba.conf\n</code></pre> <p>Adicionamos a seguinte linha:</p> <pre><code>host    all             all             172.16.0.0/20          trust\n</code></pre> <p>Ap\u00f3s essas configura\u00e7\u00f5es, sa\u00edmos do usu\u00e1rio <code>postgres</code> e liberamos a porta do banco no firewall:</p> <pre><code>$ sudo ufw allow 5432/tcp\n</code></pre> <p>Por fim, reiniciamos o servi\u00e7o para aplicar as altera\u00e7\u00f5es:</p> <pre><code>$ sudo systemctl restart postgresql\n</code></pre> <p>Essa configura\u00e7\u00e3o garantiu que o banco de dados estivesse pronto para ser utilizado pela aplica\u00e7\u00e3o Django em um ambiente bare-metal dentro da nossa nuvem MaaS.</p> <ol> <li>Funcionando e seu Status est\u00e1 como \"Ativo\" para o Sistema Operacional</li> </ol> <p>Usamos o comando: <pre><code>sudo systemctl status postgresql\n</code></pre> para verificar o status do  PostgreSQL  2. Acessivel na pr\u00f3pria maquina na qual ele foi implantado. Para verificar que esta acessivel na pr\u00f3pria maquina na qual ele foi implantado foi utilizado o comando: <pre><code>psql -U cloud -d tasks -h localhost\n</code></pre>  3. Acessivel a partir de uma conex\u00e3o vinda da m\u00e1quina MAIN. E para verificar que esta acessivel a partir de uma conex\u00e3o vinda da m\u00e1quina MAIN: <pre><code>telnet [IP do server1] 5432\n</code></pre></p> <p> 4. Em qual porta este servi\u00e7o est\u00e1 funcionando.</p> <p>Finalmente verificamos em qual porta o PostgreSQL est\u00e1 rodando com: <pre><code>sudo ss -tulnp | grep postgres\n</code></pre></p> <p></p>"},{"location":"roteiro1/main/#parte-2-implementacao-manual-da-aplicacao-django-e-banco-de-dados","title":"Parte 2: Implementa\u00e7\u00e3o Manual da Aplica\u00e7\u00e3o Django e Banco de Dados","text":"<p>Configura\u00e7\u00e3o do MaaS</p> <p>Para iniciar, acessei o MaaS e fiz login utilizando o CLI com o seguinte comando:</p> <pre><code>maas login [meu_usuario] http://172.16.0.3:5240/MAAS/\n</code></pre> <p>Em seguida, obtive o token no dashboard dentro das configura\u00e7\u00f5es do usu\u00e1rio. Com o token em m\u00e3os, solicitei a reserva de uma m\u00e1quina no MaaS:</p> <pre><code>maas [meu_usuario] machines allocate name=server2\n</code></pre> <p>A resposta retornou um JSON contendo v\u00e1rias informa\u00e7\u00f5es, entre elas o <code>system_id</code>, que anotei para uso posterior.</p> <p>Deploy da M\u00e1quina e Instala\u00e7\u00e3o da Aplica\u00e7\u00e3o</p> <p>Com o <code>system_id</code>, iniciei o deploy da m\u00e1quina via CLI:</p> <pre><code>maas [meu_usuario] machine deploy [system_id]\n</code></pre> <p>Assim que o deploy foi conclu\u00eddo, acessei a m\u00e1quina <code>server2</code> via SSH e clonei o reposit\u00f3rio do projeto Django:</p> <pre><code>git clone https://github.com/raulikeda/tasks.git\n</code></pre> <p>Naveguei at\u00e9 o diret\u00f3rio <code>tasks</code> e executei o script de instala\u00e7\u00e3o:</p> <pre><code>cd tasks\n./install.sh\n</code></pre> <p>Ap\u00f3s a instala\u00e7\u00e3o, reiniciei a m\u00e1quina e aguardei o processo de inicializa\u00e7\u00e3o.</p> <p>Teste do Servi\u00e7o</p> <p>Como server1 n\u00e3o estava registrado no DNS, adicionei-o manualmente ao arquivo <code>/etc/hosts</code> do server2: Apos a m\u00e1quina reiniciar adicionei manualmente o IP no <code>/etc/hosts</code></p> <pre><code>[IP_DO_SERVER1]  server1\n</code></pre> <p>Esta configura\u00e7\u00e3o permite que o server2 resolva o nome <code>server1</code> para o endere\u00e7o IP <code>192.168.1.100</code>, mesmo sem uma entrada correspondente no servidor DNS.</p> <p>Note</p> <p>Esta altera\u00e7\u00e3o \u00e9 local apenas para server2 e n\u00e3o afeta outros sistemas na rede.</p> <p>Para validar que a aplica\u00e7\u00e3o estava rodando corretamente, realizei um teste de acesso ao servi\u00e7o na porta 8080 diretamente do terminal do MaaS:</p> <pre><code>wget http://[IP_server2]:8080/admin/\n</code></pre> <p>Se o acesso fosse bem-sucedido, a interface de administra\u00e7\u00e3o do Django estaria funcionando corretamente.</p> <p>Exposi\u00e7\u00e3o do Servi\u00e7o via T\u00fanel SSH</p> <p>Para acessar a aplica\u00e7\u00e3o no navegador sem precisar configurar NAT no roteador, utilizei um t\u00fanel SSH. Primeiro, desconectei do SSH do MaaS e reconectei utilizando o seguinte comando:</p> <pre><code>ssh cloud@10.103.0.X -L 8001:[IP_server2]:8080\n</code></pre> <p>Esse comando criou um t\u00fanel, redirecionando o servi\u00e7o rodando na porta 8080 do <code>server2</code> para a porta 8001 no meu localhost. Certifiquei-me de que a porta 8001 n\u00e3o estava em uso antes de rodar o comando.</p> <p>Ent\u00e3o, acessei o Django Admin pelo navegador no seguinte endere\u00e7o:</p> <pre><code>http://localhost:8001/admin/\n</code></pre> <p>Fiz login com as credenciais padr\u00e3o:</p> <ul> <li>Usu\u00e1rio: cloud</li> <li>Senha: cloud</li> </ul> <p>Tarefa 2</p> <p>1.Dashboard do MAAS com as m\u00e1quinas.  2. Da aba images, com as imagens sincronizadas.  3. Aba de cada maquina(5x) mostrando os testes de hardware e commissioning com Status \"OK\"  </p>"},{"location":"roteiro1/main/#parte-3-status-dos-servidores","title":"Parte 3: Status dos servidores","text":"<p>Tarefa-3</p> <p>Prints Comprovando a Configura\u00e7\u00e3o</p> <ul> <li>Capturamos um print da tela do Dashboard do MAAS, mostrando as duas m\u00e1quinas configuradas com seus respectivos IPs. </li> <li>Registramos tamb\u00e9m um print da aplica\u00e7\u00e3o Django em execu\u00e7\u00e3o, comprovando a conex\u00e3o com o servidor. </li> <li>Explique como foi feita a implementacao manual da aplicacao Django e banco de dados.</li> </ul> <p>Implementa\u00e7\u00e3o Manual da Aplica\u00e7\u00e3o Django e Banco de Dados</p> <p>Inicialmente, configuramos uma \u00fanica aplica\u00e7\u00e3o Django. No entanto, percebemos a necessidade de expandir para dois servidores de aplica\u00e7\u00e3o (server2 e server3), ambos conectados a um \u00fanico banco de dados hospedado no server1. Essa estrat\u00e9gia foi adotada por dois motivos principais:</p> <ol> <li>Alta Disponibilidade: Caso um dos n\u00f3s falhe, o outro continua operacional, garantindo acesso cont\u00ednuo aos usu\u00e1rios.</li> <li>Balanceamento de Carga: Distribu\u00edmos as requisi\u00e7\u00f5es entre os servidores, melhorando a escalabilidade e o desempenho da aplica\u00e7\u00e3o.</li> </ol> <p>Para otimizar esse processo, decidimos automatizar a instala\u00e7\u00e3o e configura\u00e7\u00e3o usando o Ansible. Essa ferramenta facilitou a replica\u00e7\u00e3o do ambiente em m\u00faltiplos servidores, garantindo consist\u00eancia e reduzindo erros manuais.</p>"},{"location":"roteiro1/main/#parte-4-utilizando-o-ansible-deploy-automatizado-de-aplicacao","title":"Parte 4: Utilizando o Ansible - Deploy Automatizado de Aplica\u00e7\u00e3o","text":"<p>Configura\u00e7\u00e3o do Server3 e Deploy com Ansible</p> <p>Ap\u00f3s  para o maas via cli, igual feito anteriormente para o server2, realizamos a instala\u00e7\u00e3o e configura\u00e7\u00e3o do Ansible no servidor principal (main). Utilizamos o seguinte procedimento:</p> <ol> <li>Instalamos o Ansible:    <pre><code>sudo apt install ansible\n</code></pre></li> <li>Baixamos o playbook de instala\u00e7\u00e3o da aplica\u00e7\u00e3o Django:    <pre><code>wget https://raw.githubusercontent.com/raulikeda/tasks/master/tasks-install-playbook.yaml\n</code></pre></li> <li>Executamos o playbook para provisionar o server3 automaticamente:    <pre><code>ansible-playbook tasks-install-playbook.yaml --extra-vars server=[IP server3]\n</code></pre></li> </ol> <p>O Ansible garantiu que todos os procedimentos fossem executados de forma idempotente, ou seja, pod\u00edamos repetir o processo sem impactar negativamente o ambiente. Al\u00e9m disso, possibilitou a configura\u00e7\u00e3o simult\u00e2nea de m\u00faltiplos servidores, facilitando a expans\u00e3o da infraestrutura.</p> <p>Tarefa 4</p> <ol> <li>Tela do Dashboard do MAAS com as 3 Maquinas e seus respectivos IPs. </li> <li>Aplicacao Django, provando que voce est\u00e1 conectado ao server2  </li> <li>Aplicacao Django, provando que voce est\u00e1 conectado ao server3  </li> <li>Diferenca entre instalar manualmente a aplicacao Django e utilizando o Ansible.</li> </ol> <p>Com essa abordagem, conseguimos implementar uma solu\u00e7\u00e3o escal\u00e1vel e confi\u00e1vel para nossa aplica\u00e7\u00e3o Django. A automa\u00e7\u00e3o proporcionada pelo Ansible simplificou o gerenciamento dos servidores e garantiu que a aplica\u00e7\u00e3o estivesse sempre dispon\u00edvel e distribu\u00edda adequadamente.</p>"},{"location":"roteiro1/main/#parte-4-balanceamento-de-carga-com-nginx","title":"Parte 4: Balanceamento de Carga com NGINX","text":"<p>Nossa Implementa\u00e7\u00e3o com NGINX</p> <p>Instala\u00e7\u00e3o do NGINX</p> <p>Primeiramentea foi feito o deploy e instala\u00e7\u00e3o do NGINX no servidor <code>server4</code> para atuar como nosso balanceador de carga:</p> <pre><code>sudo apt update\nsudo apt install nginx -y\n</code></pre> <p>Configura\u00e7\u00e3o do M\u00f3dulo Upstream</p> <p>Para configurar o balanceamento round robin, utilizamos o m\u00f3dulo upstream do NGINX. Editamos o arquivo de configura\u00e7\u00e3o do site padr\u00e3o:</p> <pre><code>sudo nano /etc/nginx/sites-available/default\n</code></pre> <p>Adicionamos a configura\u00e7\u00e3o do balanceamento de carga da seguinte forma:</p> <pre><code>upstream backend {\n    server [IP_DO_SERVER2] :8080;\n    server [IP_DO_SERVER3] :8080;\n}\n\n\nserver {\n    location / {\n        proxy_pass http://backend;\n    }\n}\n</code></pre> <p>Ap\u00f3s salvar as altera\u00e7\u00f5es, reiniciamos o servi\u00e7o do NGINX:</p> <p><pre><code>sudo service nginx restart\n</code></pre> Tarefa 5</p> <ol> <li> <p>De um print da tela do Dashboard do MAAS com as 4 Maquinas e seus respectivos IPs.  Personaliza\u00e7\u00e3o das Aplica\u00e7\u00f5es Django</p> </li> <li> <p>Alteramos o conte\u00fado da mensagem contida na fun\u00e7\u00e3o <code>index</code> do arquivo <code>tasks/views.py</code> de cada server para distinguir ambos os servers. </p> </li> </ol> <p>Para podermos identificar facilmente qual servidor estava respondendo a cada requisi\u00e7\u00e3o, modificamos o arquivo <code>tasks/views.py</code> em cada inst\u00e2ncia Django, alterando a mensagem de resposta para algo \u00fanico em cada servidor:</p> <p><pre><code># Em server1\nfrom django.shortcuts import render\nfrom django.http import HttpResponse\n\ndef index(request):\n    return HttpResponse(\"Hello, world. You're at server 2.\")\n</code></pre> </p> <p><pre><code># Em server2\nfrom django.shortcuts import render\nfrom django.http import HttpResponse\n\ndef index(request):\n    return HttpResponse(\"Hello, world. You're at server 2.\")\n</code></pre> </p> <ol> <li> <p>Fa\u00e7a um <code>GET request</code> para o path que voce criou em urls.py para o Nginx e tire 2 prints das respostas de cada request, provando que voce est\u00e1 conectado ao server 4, que \u00e9 o Proxy Reverso e que ele bate cada vez em um server diferente server2 e server3.</p> </li> <li> <p>As respostas de cada request do server 4, mostram que cada vez que um <code>GET request</code> \u00e9 feito ele bate cada vez em um server diferente server2 e server3.  </p> </li> </ol>"},{"location":"roteiro2/main/","title":"Relat\u00f3rio - Roteiro 2: Gerenciamento de Aplica\u00e7\u00f5es com Juju","text":""},{"location":"roteiro2/main/#objetivo","title":"Objetivo","text":"<p>O objetivo deste roteiro \u00e9 explorar o orquestrador Juju para gerenciar aplica\u00e7\u00f5es em ambientes de nuvem. Atrav\u00e9s do Juju, buscamos:</p> <ol> <li>Simplificar o deploy e gerenciamento de aplica\u00e7\u00f5es complexas  </li> <li>Implementar solu\u00e7\u00f5es de monitoramento com Prometheus e Grafana  </li> <li>Gerenciar relacionamentos entre servi\u00e7os de forma declarativa  </li> <li>Automatizar a configura\u00e7\u00e3o de ambientes escal\u00e1veis  </li> </ol>"},{"location":"roteiro2/main/#infra","title":"Infra","text":""},{"location":"roteiro2/main/#configuracao-inicial-do-ambiente-juju","title":"Configura\u00e7\u00e3o Inicial do Ambiente Juju","text":"<ol> <li>Instala\u00e7\u00e3o do Juju: <pre><code>sudo snap install juju --channel 3.6\n</code></pre> Depois, verificamos que o Juju reconhece o MaaS como provedor de recursos com: <pre><code>juju clouds\n</code></pre></li> </ol> <p>Como o MaaS n\u00e3o apareceu na lista de clouds do Juju, adicionamos o manualmente usando um arquivo YAML.</p> <p><pre><code>nano maas-cloud.yaml\n</code></pre> <pre><code>clouds:\n  my-maas:\n    type: maas\n    auth-types: [oauth1]\n    endpoint: http://&lt;SEU_IP_MAAS&gt;:5240/MAAS\n</code></pre> <pre><code>juju add-cloud --client my-maas maas-cloud.yaml\n</code></pre></p> <p>Adicionamos as credenciais MAAS para que o Juju possa interagir com a nova cloud adicionada. <pre><code>credentials:\n  maas-one:\n    anyuser:\n      auth-type: oauth1\n      maas-oauth: &lt;API KEY&gt;\n</code></pre> <pre><code>juju add-credential --client -f maas-creds.yaml maas-one\n</code></pre></p>"},{"location":"roteiro2/main/#criacao-do-controlador","title":"Cria\u00e7\u00e3o do Controlador","text":"<p>Para criar o controlador no <code>server1</code>, primeiro colocamos a tag \"juju\" na m\u00e1quina pelo painel do MAAS. Depois, executamos:</p> <pre><code>juju bootstrap --bootstrap-series=jammy --constraints tags=juju maas-one maas-controller\n</code></pre> <p>Esse processo pode demorar, pois o Juju precisa provisionar uma m\u00e1quina, instalar pacotes e configurar servi\u00e7os. Para acompanhar o progresso, utilizamos e deixaremos esse terminal aberto daqui em diante:</p> <pre><code>watch -n 1 --color 'juju status --color'\n</code></pre>"},{"location":"roteiro2/main/#app","title":"App","text":"<p><pre><code>juju add-model --config default-series=jammy openstack\njuju switch controller\njuju deploy juju-dashboard --to lxd:0\njuju intergrate juju-dashboard controller\n</code></pre> Cria o modelo openstack com a s\u00e9rie padr\u00e3o jammy. Troca para o modelo controller. Implanta o painel do Juju (juju-dashboard) em um container da m\u00e1quina 0. Integra o painel ao controlador para gerenciar os modelos via interface web.</p> <p><pre><code>juju enable-dashboard\njuju dashboard\n</code></pre> Ativa a interface web do Juju. Exibe o link de acesso e credenciais para login.</p> <p><pre><code>mkdir -p /home/cloud/charms\ncd /home/cloud/charms\n</code></pre> Cria e acessa o diret\u00f3rio onde os charms ser\u00e3o armazenados.</p> <p><pre><code>juju download grafana\njuju download prometheus2\n</code></pre> Baixa os charms Grafana e Prometheus2 localmente para futura implanta\u00e7\u00e3o.</p> <pre><code>juju switch openstack\njuju deploy ./prometheus2_r60.charm\njuju deploy grafana\n</code></pre> <p>Troca para o modelo openstack. Implanta o Prometheus2 a partir do arquivo .charm baixado. Implanta o Grafana diretamente do reposit\u00f3rio de charms.</p> <p><pre><code>juju intergrate grafana:grafana-source prometheus2:grafana-source\n</code></pre> Conecta Grafana ao Prometheus, permitindo uso das m\u00e9tricas como fonte de dados.</p> <p><pre><code>juju expose grafana\njuju expose prometheus2\n</code></pre> Libera acesso externo aos servi\u00e7os via rede.</p> <p>Acessamos o Grafana via navegador:</p> <p>Obtemos o IP da m\u00e1quina do grafana com juju status</p> <p>Acessamos http://:3000 e fizemos login com a senha resultante dessa linha <p><pre><code>juju run grafana/0 get-admin-password\n</code></pre> Fa\u00e7a login e configure seu dashboard!</p>"},{"location":"roteiro2/main/#tarefas","title":"Tarefas","text":""},{"location":"roteiro3/main/","title":"Roteiro 3 - OpenStack","text":""},{"location":"roteiro3/main/#objetivo","title":"Objetivo","text":"<p>Este roteiro tem como objetivo configurar e utilizar o ambiente OpenStack para integra\u00e7\u00e3o com Docker e Kubernetes, possibilitando gerenciamento eficiente de cont\u00eaineres e garantindo escalabilidade e alta disponibilidade dos servi\u00e7os.</p>"},{"location":"roteiro3/main/#tarefa-1","title":"Tarefa 1","text":""},{"location":"roteiro3/main/#status-juju","title":"Status JUJU","text":"<p>O JUJU foi utilizado para gerenciar as m\u00e1quinas f\u00edsicas e virtuais. A imagem abaixo apresenta o status das aplica\u00e7\u00f5es e servi\u00e7os gerenciados pelo JUJU.</p> <p></p>"},{"location":"roteiro3/main/#dashboard-do-maas-com-maquinas","title":"Dashboard do MAAS com m\u00e1quinas","text":"<p>O MAAS (Metal as a Service) mostra as m\u00e1quinas f\u00edsicas e seu status, indicando quais est\u00e3o dispon\u00edveis para utiliza\u00e7\u00e3o.</p> <p></p>"},{"location":"roteiro3/main/#aba-compute-overview-no-openstack","title":"Aba Compute Overview no OpenStack","text":"<p>Nesta aba temos uma vis\u00e3o geral do ambiente computacional do OpenStack, incluindo recursos utilizados e dispon\u00edveis.</p> <p></p>"},{"location":"roteiro3/main/#aba-compute-instances-no-openstack","title":"Aba Compute Instances no OpenStack","text":"<p>Aqui s\u00e3o apresentadas as inst\u00e2ncias virtuais ativas no ambiente, incluindo detalhes sobre recursos alocados e status.</p> <p></p>"},{"location":"roteiro3/main/#aba-network-topology-no-openstack","title":"Aba Network Topology no OpenStack","text":"<p>Esta aba ilustra a topologia da rede criada no OpenStack, evidenciando conex\u00f5es, redes e inst\u00e2ncias.</p> <p></p>"},{"location":"roteiro3/main/#tarefa-2","title":"Tarefa 2","text":""},{"location":"roteiro3/main/#dashboard-do-maas-com-as-maquinas","title":"Dashboard do MAAS com as m\u00e1quinas","text":"<p>Ap\u00f3s modifica\u00e7\u00f5es, o MAAS mostra as m\u00e1quinas f\u00edsicas com ajustes nas aloca\u00e7\u00f5es.</p> <p></p>"},{"location":"roteiro3/main/#aba-compute-overview-no-openstack_1","title":"Aba Compute Overview no OpenStack","text":"<p>Esta aba atualizada do OpenStack permite observar altera\u00e7\u00f5es no uso de recursos computacionais.</p> <p></p>"},{"location":"roteiro3/main/#aba-compute-instances-no-openstack_1","title":"Aba Compute Instances no OpenStack","text":"<p>A aba mostra novas inst\u00e2ncias ou altera\u00e7\u00f5es em inst\u00e2ncias anteriores.</p> <p></p>"},{"location":"roteiro3/main/#aba-network-topology-no-openstack_1","title":"Aba Network Topology no OpenStack","text":"<p>A nova topologia de rede reflete altera\u00e7\u00f5es feitas no ambiente.</p> <p></p> <p>Diferen\u00e7as observadas entre Tarefa 1 e Tarefa 2: - Foram identificadas diferen\u00e7as principalmente na configura\u00e7\u00e3o das inst\u00e2ncias, utiliza\u00e7\u00e3o de recursos e poss\u00edveis altera\u00e7\u00f5es na rede ap\u00f3s configura\u00e7\u00e3o adicional.</p> <p>Recursos criados e modificados: - Inst\u00e2ncias virtuais, recursos computacionais e ajustes na topologia de rede para suportar maior carga ou diferentes requisitos.</p>"},{"location":"roteiro3/main/#tarefa-3","title":"Tarefa 3","text":""},{"location":"roteiro3/main/#arquitetura-de-rede-com-sub-redes","title":"Arquitetura de rede com sub-redes","text":"<p>A imagem a seguir representa toda a arquitetura de rede configurada, desde o acesso do usu\u00e1rio no Insper at\u00e9 a inst\u00e2ncia final alocada no OpenStack. Ela mostra claramente a transi\u00e7\u00e3o entre redes (Rede Insper, VPN, Internet), a entrada pelo roteador do OpenStack e a separa\u00e7\u00e3o entre rede externa e interna. Nessa rede interna, temos todas as VMs alocadas, como Load Balancer, inst\u00e2ncias de aplica\u00e7\u00e3o, banco de dados e cliente. As sub-redes utilizadas tamb\u00e9m est\u00e3o indicadas explicitamente para melhor entendimento da estrutura l\u00f3gica.</p> <p></p>"},{"location":"roteiro3/main/#tarefa-4","title":"Tarefa 4","text":""},{"location":"roteiro3/main/#relatorio-dos-passos-utilizados","title":"Relat\u00f3rio dos passos utilizados","text":"<p>Nesta tarefa foram executados os seguintes passos detalhados:</p> <ol> <li>Instala\u00e7\u00e3o e configura\u00e7\u00e3o inicial do OpenStack, incluindo Octavia para balanceamento de carga.</li> <li>Cria\u00e7\u00e3o da arquitetura de rede necess\u00e1ria para o ambiente.</li> <li>Defini\u00e7\u00e3o e configura\u00e7\u00e3o de inst\u00e2ncias virtuais (VMs) com nomes e IPs espec\u00edficos.</li> <li>Implementa\u00e7\u00e3o de uma aplica\u00e7\u00e3o FastAPI conectada atrav\u00e9s de Nginx/LoadBalancer.</li> <li>Aloca\u00e7\u00e3o e gerenciamento das inst\u00e2ncias em m\u00e1quinas f\u00edsicas diferentes via OpenStack.</li> </ol>"},{"location":"roteiro3/main/#arquitetura-de-rede","title":"Arquitetura de rede","text":"<p>Abaixo a imagem detalha a arquitetura de rede configurada no ambiente OpenStack, evidenciando conex\u00f5es entre servi\u00e7os, redes e inst\u00e2ncias.</p> <p></p>"},{"location":"roteiro3/main/#lista-de-vms-com-nomes-e-ips-alocados","title":"Lista de VMs com nomes e IPs alocados","text":"<p>Nesta imagem \u00e9 poss\u00edvel verificar todas as VMs configuradas, incluindo seus respectivos nomes e IPs.</p> <p></p>"},{"location":"roteiro3/main/#dashboard-do-fastapi-conectado-via-maquina-nginxlb","title":"Dashboard do FastAPI conectado via m\u00e1quina Nginx/LB","text":"<p>O dashboard do servi\u00e7o FastAPI mostra sua disponibilidade atrav\u00e9s da interface configurada pelo Nginx Load Balancer.</p> <p></p>"},{"location":"roteiro3/main/#instancias-e-suas-alocacoes-nos-servidores-maquinas-fisicas","title":"Inst\u00e2ncias e suas aloca\u00e7\u00f5es nos servidores (m\u00e1quinas f\u00edsicas)","text":"<p>As imagens seguintes ilustram em quais m\u00e1quinas f\u00edsicas cada inst\u00e2ncia virtual foi alocada:</p>"},{"location":"roteiro4/main/","title":"Roteiro 4 - OpenStack e Kubernetes: Checkpoint por Aluno","text":""},{"location":"roteiro4/main/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Neste roteiro, avan\u00e7amos na consolida\u00e7\u00e3o do dom\u00ednio do OpenStack por meio de um checkpoint individual. A proposta \u00e9 garantir que cada aluno seja capaz de navegar pelas diferentes abas da plataforma, compreendendo a l\u00f3gica de projetos, usu\u00e1rios, aloca\u00e7\u00e3o de inst\u00e2ncias, al\u00e9m de analisar a topologia de rede resultante da sua configura\u00e7\u00e3o.</p> <p>A verifica\u00e7\u00e3o parte de um conjunto m\u00ednimo de prints que comprovam o uso adequado da interface de gerenciamento e a correta associa\u00e7\u00e3o de recursos.</p>"},{"location":"roteiro4/main/#tarefa-1-a-6-verificacoes-no-painel-openstack","title":"Tarefa 1 a 6 - Verifica\u00e7\u00f5es no Painel OpenStack","text":"<p>Abaixo est\u00e3o os prints que comprovam o estado atual da minha infraestrutura dentro do OpenStack, conforme solicitado:</p>"},{"location":"roteiro4/main/#identity-aba-projects","title":"Identity - Aba \"Projects\"","text":"<p>Exibe os projetos associados ao ambiente OpenStack. Cada projeto agrupa recursos e usu\u00e1rios, permitindo o isolamento de ambientes.</p> <p></p>"},{"location":"roteiro4/main/#identity-aba-users","title":"Identity - Aba \"Users\"","text":"<p>Mostra os usu\u00e1rios cadastrados, suas fun\u00e7\u00f5es e permiss\u00f5es dentro dos projetos. Cada usu\u00e1rio tem pap\u00e9is distintos e pode interagir com recursos conforme suas permiss\u00f5es.</p> <p></p>"},{"location":"roteiro4/main/#compute-aba-overview","title":"Compute - Aba \"Overview\"","text":"<p>Painel de vis\u00e3o geral com consumo de recursos, como inst\u00e2ncias, vCPUs, RAM e armazenamento. \u00datil para controle de capacidade.</p> <p></p>"},{"location":"roteiro4/main/#compute-aba-instances","title":"Compute - Aba \"Instances\"","text":"<p>Lista detalhada de inst\u00e2ncias ativas, incluindo nome, status, IP e recursos alocados.</p> <p></p>"},{"location":"roteiro4/main/#network-aba-topology-visao-1","title":"Network - Aba \"Topology\" (Vis\u00e3o 1)","text":"<p>Apresenta visualmente a topologia de rede criada, com destaque para sub-redes, roteadores e inst\u00e2ncias conectadas.</p> <p></p>"},{"location":"roteiro4/main/#network-aba-topology-visao-2","title":"Network - Aba \"Topology\" (Vis\u00e3o 2)","text":"<p>Outra vis\u00e3o complementar da mesma topologia, demonstrando a comunica\u00e7\u00e3o entre componentes virtuais.</p> <p></p>"},{"location":"roteiro4/main/#topologia-detalhada-da-aplicacao","title":"Topologia Detalhada da Aplica\u00e7\u00e3o","text":"<p>Visualiza\u00e7\u00e3o da arquitetura montada com Kubernetes e servi\u00e7os integrados, incluindo Load Balancer, banco de dados, APIs e cliente.</p> <p> </p>"},{"location":"roteiro4/main/#plano-de-disaster-recovery-e-sla","title":"Plano de Disaster Recovery e SLA","text":"<p>Diante da responsabilidade de liderar a implanta\u00e7\u00e3o de um sistema cr\u00edtico, distribu\u00eddo em v\u00e1rias regi\u00f5es do Brasil e com alto grau de confidencialidade, a escolha da infraestrutura se d\u00e1 pela ado\u00e7\u00e3o de uma nuvem privada (Private Cloud). Isso garante controle direto sobre os dados sens\u00edveis, menor exposi\u00e7\u00e3o p\u00fablica e permite customiza\u00e7\u00f5es espec\u00edficas de seguran\u00e7a e rede.</p> <p>Para sustentar a opera\u00e7\u00e3o cont\u00ednua e minimizar falhas, a presen\u00e7a de um time DevOps \u00e9 essencial. Ele garante a automa\u00e7\u00e3o de processos, integra\u00e7\u00e3o cont\u00ednua, entrega \u00e1gil de melhorias e respostas r\u00e1pidas a incidentes. O time atua de forma colaborativa entre desenvolvimento e opera\u00e7\u00f5es, otimizando o ciclo de vida das aplica\u00e7\u00f5es.</p> <p>Pensando em resili\u00eancia e continuidade, elaboramos um plano de Disaster Recovery (DR) e Alta Disponibilidade (HA) que prev\u00ea os seguintes pilares:</p> <ul> <li> <p>Mapeamento de amea\u00e7as: Foram identificadas amea\u00e7as como falhas de hardware, erros humanos, ataques cibern\u00e9ticos (DDoS, ransomware), indisponibilidade de links de rede e falhas de energia nos datacenters. A an\u00e1lise permitiu priorizar os riscos de maior impacto e frequ\u00eancia.</p> </li> <li> <p>Plano de recupera\u00e7\u00e3o: A estrat\u00e9gia prev\u00ea snapshots autom\u00e1ticos e regulares das VMs e volumes cr\u00edticos, failover automatizado entre inst\u00e2ncias replicadas, e reimplanta\u00e7\u00f5es r\u00e1pidas via scripts de infraestrutura como c\u00f3digo (IaC). Tamb\u00e9m s\u00e3o utilizados testes peri\u00f3dicos de recupera\u00e7\u00e3o para garantir a efic\u00e1cia dos procedimentos.</p> </li> <li> <p>Pol\u00edtica de backup: A equipe estabelece backups di\u00e1rios incrementais e semanais completos, com reten\u00e7\u00e3o de no m\u00ednimo 30 dias. Os dados s\u00e3o replicados em zonas de disponibilidade diferentes, com verifica\u00e7\u00e3o automatizada da integridade.</p> </li> <li> <p>Alta disponibilidade: Load Balancers e inst\u00e2ncias em clusters replicados garantem balanceamento de carga e toler\u00e2ncia a falhas. O uso de Kubernetes permite a redistribui\u00e7\u00e3o autom\u00e1tica de pods em caso de falhas f\u00edsicas, enquanto servi\u00e7os cr\u00edticos s\u00e3o monitorados por probes de sa\u00fade.</p> </li> </ul> <p>Este plano garante que a infraestrutura possa resistir a incidentes, restaurar opera\u00e7\u00f5es em minutos e cumprir os SLAs estabelecidos com seguran\u00e7a e robustez.</p>"},{"location":"roteiro4/main/#conclusao","title":"Conclus\u00e3o","text":"<p>Este checkpoint refor\u00e7a a import\u00e2ncia do dom\u00ednio individual da plataforma OpenStack, especialmente na gest\u00e3o de usu\u00e1rios, projetos e inst\u00e2ncias. Os prints apresentados atestam a execu\u00e7\u00e3o correta das tarefas propostas, bem como a configura\u00e7\u00e3o funcional da infraestrutura de rede. O dom\u00ednio dessas opera\u00e7\u00f5es b\u00e1sicas garante uma base s\u00f3lida para a implanta\u00e7\u00e3o e manuten\u00e7\u00e3o de solu\u00e7\u00f5es em nuvem mais complexas.</p>"}]}